# ChatOllama

## Pr√©requis

1. T√©l√©chargez [Ollama](https://github.com/ollama/ollama) ou ex√©cutez-le sur [Docker.](https://hub.docker.com/r/ollama/ollama)&#x20;
2. Par exemple, vous pouvez utiliser la commande suivante pour lancer une instance Docker avec llama3

    ```bash
    docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
    docker exec -it ollama ollama run llama3
    ```

## Configuration

1. **Mod√®les de chat** > faites glisser le n≈ìud **ChatOllama**

<figure><img src="../../../.gitbook/assets/image (139).png" alt="" width="563"><figcaption></figcaption></figure>

2. Remplissez le mod√®le qui fonctionne sur Ollama. Par exemple : `llama2`. Vous pouvez √©galement utiliser des param√®tres suppl√©mentaires :

<figure><img src="../../../.gitbook/assets/image (140).png" alt=""><figcaption></figcaption></figure>

3. Voil√† [üéâ](https://emojipedia.org/party-popper/), vous pouvez maintenant utiliser le **n≈ìud ChatOllama** dans Flowise

<figure><img src="../../../.gitbook/assets/image (141).png" alt=""><figcaption></figcaption></figure>

### Suppl√©mentaire

Si vous ex√©cutez √† la fois Flowise et Ollama sur Docker, vous devrez changer l'URL de base pour ChatOllama.

Pour les syst√®mes d'exploitation Windows et MacOS, sp√©cifiez [http://host.docker.internal:8000](http://host.docker.internal:8000/). Pour les syst√®mes bas√©s sur Linux, la passerelle Docker par d√©faut doit √™tre utilis√©e car host.docker.internal n'est pas disponible : [http://172.17.0.1:8000](http://172.17.0.1:8000/)

<figure><img src="../../../.gitbook/assets/image (142).png" alt="" width="292"><figcaption></figcaption></figure>

## Ressources

* [LangchainJS ChatOllama](https://js.langchain.com/docs/integrations/chat/ollama)
* [Ollama](https://github.com/ollama/ollama)